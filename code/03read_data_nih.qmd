---
title: "Read, clean and join NIH data"
format: html
editor: source
---

# Summary of script and output datafile structure

This script downloads, processes, and joins NIH grant data from multiple sources to create an analysis dataset of NIH grants with termination information.

## Data Sources

- **NIH ExPORTER** (bulk download): FY 2014-2024 grant records with abstracts
- **NIH Reporter** (manual download): FY 2025 grant records (bulk download not yet available)
- **Grant Witness**: Termination/freeze status for affected grants

## Processing Steps

1. Download ExPORTER ZIP files for projects and abstracts (skipped if already downloaded)
2. Extract and read CSVs, filter to US grants only
3. Join abstracts to main dataset
4. Read and standardize FY 2025 manual downloads
5. Combine all years (2014-2025) into unified dataset
6. Join with Grant Witness termination data
7. Save analysis dataset

## Caching

The script caches intermediate results to avoid reprocessing:

- `data/nih_exporter_usa_2014_2024.rds`: ExPORTER data with abstracts
- `data/nih_exporter_usa_2014_2025.rds`: Combined 2014-2025 data

Delete cache files to force reprocessing.

## Output Files

- `data_analysis/nih_analysis.csv`: CSV format for broad compatibility
- `data_analysis/nih_analysis.parquet`: Parquet format for Python compatibility (and faster R loading)

## Dataset Statistics

| Metric         | Count   |
|----------------|---------|
| Total rows     | ~230,000 |
| Total columns  | 38      |
| Cut grants     | ~5,800   |
| Non-cut grants | ~224,000 |
| Fiscal years   | 2014-2025 |

## Column Structure (38 columns)

**From ExPORTER (21 columns)** - authoritative grant data:

- IDs: application_id, core_project_num
- Agency (IC = Institute or Center, organizational units of the NIH): administering_ic, administering_ic_name, study_section, study_section_name
- PI: pi_names
- Organization: org_name, org_state, congressional_district, org_dept, organization_type
- Project: project_title, project_terms, public_health_relevance, nih_spending_categorization, project_abstract
- Dates: fiscal_year, project_start, project_end
- Budget: total_cost

**From Grant Witness (17 columns)** - termination-specific:

- Status: status, ever_frozen
- Dates: targeted_start_date, targeted_end_date, frozen_date, unfrozen_date, termination_date
- Reinstatement: reinstatement_indicator, reinstated_est_date
- Spending: total_estimated_outlays, total_estimated_remaining, last_payment_date, cancellation_source
- Analysis: flagged_words
- Political: us_rep
- URLs: usaspending_url, reporter_url

**Note:** Cut grants have non-NA `status`. Use `!is.na(status)` to filter.

# CODE

## Packages and settings

```{r}
# Packages
library(tidyverse)
library(lubridate)  # for date parsing (mdy)
library(here)
library(arrow) # for writing data to .parquet format for python

# To load the end product of this script, load nih_joined here (also possible to load from .csv)
nih_joined <- arrow::read_parquet(here('data_analysis/nih_analysis.parquet'))

# Settings for this script
fiscal_years <- 2014:2024 # (2025 and 2026 don't exist as bulk download)
data_dir <- here("data_raw/nih_exporter")

# Create directory if it doesn't exist
if (!dir.exists(data_dir)) {
  dir.create(data_dir, recursive = TRUE)
}
```

## Load cached data

Load the most complete cached dataset available. This makes `nih_exporter_usa` available for inspection even if you don't want to reprocess everything. For later runs of this script, once data has been downloaded and processed a first time. 

```{r load-cache}
# Define cache file paths
exporter_cache <- here("data/nih_exporter_usa_2014_2024.rds")
combined_cache <- here("data/nih_exporter_usa_2014_2025.rds")

# Check what caches exist and load the most complete one
if (file.exists(combined_cache)) {
  nih_exporter_usa <- readRDS(combined_cache)
  cat("Loaded combined 2014-2025 cache:", nrow(nih_exporter_usa), "rows\n")
  cache_status <- "combined"
} else if (file.exists(exporter_cache)) {
  nih_exporter_usa <- readRDS(exporter_cache)
  cat("Loaded 2014-2024 cache:", nrow(nih_exporter_usa), "rows\n")
  cat("(2025 data not yet merged - run sections below to add it)\n")
  cache_status <- "exporter_only"
} else {
  cat("No cache found. Run all sections below to create data.\n")
  cache_status <- "none"
}

# Set processing flags based on cache status
skip_exporter_processing <- cache_status %in% c("exporter_only", "combined")
skip_2025_processing <- cache_status == "combined"

cat("\nProcessing flags:\n")
cat("  skip_exporter_processing:", skip_exporter_processing, "\n")
cat("  skip_2025_processing:", skip_2025_processing, "\n")
```

## Download NIH ExPORTER data

- ExPORTER provides bulk downloads of NIH grant data by fiscal year.
 -Source: https://reporter.nih.gov/exporter/
- Abstract files (download-abstracts chunk) - from https://reporter.nih.gov/exporter/abstracts/download/<fy> for 2014-2024

```{r download-exporter}
# Download ExPORTER project files for fiscal years 2014-2026
# Files are ZIP archives containing CSV data
# URL format: https://reporter.nih.gov/exporter/projects/download/<fy>

# Download each fiscal year (skip if already downloaded)
for (fy in fiscal_years) {
  zip_file <- file.path(data_dir, paste0("RePORTER_PRJ_C_FY", fy, ".zip"))

  if (!file.exists(zip_file)) {
    url <- paste0("https://reporter.nih.gov/exporter/projects/download/", fy)
    cat("Downloading FY", fy, "...\n")

    # Try download, handle potential errors
    tryCatch({
      download.file(url, zip_file, mode = "wb", quiet = FALSE)
      cat("  Downloaded successfully\n")
    }, error = function(e) {
      cat("  Error downloading FY", fy, ":", e$message, "\n")
    })

    # Be polite to the server
    Sys.sleep(2)
  } else {
    cat("FY", fy, "already exists, skipping\n")
  }
}

# List downloaded files
list.files(data_dir, pattern = "\\.zip$")
```

```{r download-abstracts}
# Download ExPORTER abstract files for fiscal years 2014-2024
# URL format: https://reporter.nih.gov/exporter/abstracts/download/<fy>

for (fy in fiscal_years) {
  zip_file <- file.path(data_dir, paste0("RePORTER_PRJABS_C_FY", fy, ".zip"))

  if (!file.exists(zip_file)) {
    url <- paste0("https://reporter.nih.gov/exporter/abstracts/download/", fy)
    cat("Downloading abstracts FY", fy, "...\n")

    tryCatch({
      download.file(url, zip_file, mode = "wb", quiet = FALSE)
      cat("  Downloaded successfully\n")
    }, error = function(e) {
      cat("  Error downloading abstracts FY", fy, ":", e$message, "\n")
    })

    Sys.sleep(2)
  } else {
    cat("Abstracts FY", fy, "already exists, skipping\n")
  }
}

# List abstract files
list.files(data_dir, pattern = "PRJABS.*\\.zip$")
```


## Extract, read and join bulk download EXPORTER datasets

```{r extract-and-read}
# Extract and read all CSV files from the ZIP archives
# Filter for US grants only (Organization Country == "UNITED STATES")
# Skip if cache was loaded in load-cache section above

if (skip_exporter_processing) {
  cat("Using cached data (loaded above). Delete cache files to reprocess.\n")
}

read_exporter_year <- function(fy, data_dir) {
  zip_file <- file.path(data_dir, paste0("RePORTER_PRJ_C_FY", fy, ".zip"))

  if (!file.exists(zip_file)) {
    cat("ZIP file for FY", fy, "not found\n")
    return(NULL)
  }

  # List contents of ZIP file
  zip_contents <- unzip(zip_file, list = TRUE)
  csv_file <- zip_contents$Name[grepl("\\.csv$", zip_contents$Name)]

  if (length(csv_file) == 0) {
    cat("No CSV found in FY", fy, "ZIP\n")
    return(NULL)
  }

  # Extract to temp location and read
  temp_dir <- tempdir()
  unzip(zip_file, files = csv_file, exdir = temp_dir, overwrite = TRUE)

  # Read CSV
  df <- read_csv(file.path(temp_dir, csv_file), show_col_types = FALSE)

  # Ensure consistent column types across years (some columns are empty in certain years)
  if ("SUBPROJECT_ID" %in% names(df)) {
    df$SUBPROJECT_ID <- as.character(df$SUBPROJECT_ID)
  }

  # Add fiscal year column if not present
  if (!"FISCAL_YEAR" %in% names(df)) {
    df$FISCAL_YEAR <- fy
  }

  cat("FY", fy, ":", nrow(df), "rows,", ncol(df), "columns\n")
  return(df)
}

# Read all fiscal years (only if not using cache)
if (!skip_exporter_processing) {
  nih_exporter_list <- lapply(fiscal_years, read_exporter_year, data_dir = data_dir)
  # Remove NULL entries (failed downloads)
  nih_exporter_list <- nih_exporter_list[!sapply(nih_exporter_list, is.null)]
}
```

```{r combine-datasets}
# Combine all years into one dataset (skip if using cache)
if (!skip_exporter_processing) {
  nih_exporter_all <- bind_rows(nih_exporter_list)

  cat("\nCombined dataset:\n")
  cat("  Total rows:", nrow(nih_exporter_all), "\n")
  cat("  Total columns:", ncol(nih_exporter_all), "\n")

  # Filter for United States only
  nih_exporter_usa <- nih_exporter_all %>%
    filter(ORG_COUNTRY == "UNITED STATES")

  cat("\nUS grants only:\n")
  cat("  Total rows:", nrow(nih_exporter_usa), "\n")

  # Summary by fiscal year
  cat("\nGrants per fiscal year:\n")
  print(table(nih_exporter_usa$FISCAL_YEAR))

  # Clean up memory
  rm(nih_exporter_list, nih_exporter_all)
  gc()
}
```

```{r extract-and-read-abstracts}
# Unzip and Read abstract files and join with main data
# Abstract files contain APPLICATION_ID and ABSTRACT_TEXT
# Skip if using cached data (abstracts already joined in cache)

if (skip_exporter_processing) {
  cat("Skipping abstract extraction (using cached data with abstracts)\n")
} else {

read_abstracts_year <- function(fy, data_dir) {
  zip_file <- file.path(data_dir, paste0("RePORTER_PRJABS_C_FY", fy, ".zip"))

  if (!file.exists(zip_file)) {
    cat("Abstract ZIP file for FY", fy, "not found\n")
    return(NULL)
  }

  # List contents of ZIP file
  zip_contents <- unzip(zip_file, list = TRUE)
  csv_file <- zip_contents$Name[grepl("\\.csv$", zip_contents$Name)]

  if (length(csv_file) == 0) {
    cat("No CSV found in abstracts FY", fy, "ZIP\n")
    return(NULL)
  }

  # Extract to temp location and read
  temp_dir <- tempdir()
  unzip(zip_file, files = csv_file, exdir = temp_dir, overwrite = TRUE)

  # Read CSV
  df <- read_csv(file.path(temp_dir, csv_file), show_col_types = FALSE)

  cat("Abstracts FY", fy, ":", nrow(df), "rows\n")
  return(df)
}

# Read all abstract files
  abstracts_list <- lapply(fiscal_years, read_abstracts_year, data_dir = data_dir)
  abstracts_list <- abstracts_list[!sapply(abstracts_list, is.null)]

  # Combine all abstracts
  abstracts_all <- bind_rows(abstracts_list)
  cat("\nTotal abstracts:", nrow(abstracts_all), "\n")

  # Clean up
  rm(abstracts_list)
} # end skip_exporter_processing check
```

```{r join-abstracts}
# Join abstracts to main ExPORTER data by APPLICATION_ID
# Data was already loaded in load-cache section if cache exists

if (skip_exporter_processing) {
  cat("Using cached ExPORTER data (already loaded above):\n")
  cat("  Total rows:", nrow(nih_exporter_usa), "\n")
  # Check for abstract column (may be lowercase if from combined cache)
  abstract_col <- if ("ABSTRACT_TEXT" %in% names(nih_exporter_usa)) "ABSTRACT_TEXT" else "abstract_text"
  cat("  Grants with abstracts:", sum(!is.na(nih_exporter_usa[[abstract_col]])), "\n")
} else {
  # Join abstracts to main ExPORTER data
  nih_exporter_usa <- nih_exporter_usa %>%
    left_join(
      abstracts_all %>% select(APPLICATION_ID, ABSTRACT_TEXT),
      by = "APPLICATION_ID"
    )

  cat("After joining abstracts:\n")
  cat("  Total rows:", nrow(nih_exporter_usa), "\n")
  cat("  Grants with abstracts:", sum(!is.na(nih_exporter_usa$ABSTRACT_TEXT)), "\n")
  cat("  Grants without abstracts:", sum(is.na(nih_exporter_usa$ABSTRACT_TEXT)), "\n")

  # Clean up
  rm(abstracts_all)
  gc()

  # Save checkpoint: ExPORTER 2014-2024 data with abstracts
  saveRDS(nih_exporter_usa, exporter_cache)
  cat("Saved cache to:", exporter_cache, "\n")
}
```

## Read manually downloaded 2025 NIH Reporter data

NIH Reporter manual downloads (FY 2025) were split by agency groups to avoid the 15,000 record download limit. Files include agencies in the filename.

```{r read-manual-2025}
# Skip 2025 processing if combined cache was loaded (flag set in load-cache section)

if (skip_2025_processing) {
  cat("Skipping 2025 manual data processing (will load combined cache)\n")
} else {

# Find all 2025 manual download CSV files
manual_2025_files <- list.files(
  data_dir,
  pattern = "nih_exporter_year2025.*\\.csv$",
  full.names = TRUE
)

cat("Found", length(manual_2025_files), "manual 2025 files:\n")
print(basename(manual_2025_files))

# Function to read a manual 2025 download file
# These files have 7 header rows before the column names
read_manual_2025 <- function(file_path) {
  # Read skipping the first 7 rows (header metadata)
  df <- read_csv(file_path, skip = 7, show_col_types = FALSE)

  # Comprehensive rename: map NIH Reporter column names to ExPORTER bulk format
  df <- df %>%
    rename(
      # IDs
      APPLICATION_ID = `Application ID`,
      FULL_PROJECT_NUM = `Project Number`,
      SERIAL_NUMBER = `Serial Number`,
      SUBPROJECT_ID = `Subproject Number`,
      SUFFIX = `Suffix`,
      SUPPORT_YEAR = `Support Year`,
      APPLICATION_TYPE = `Type`,
      # Agency/Admin
      ADMINISTERING_IC = `Administering IC`,
      IC = `IC`,
      FUNDING_ICs = `Funding IC(s)`,
      FUNDING_MECHANISM = `Funding Mechanism`,
      FOA_NUMBER = `Opportunity Number`,
      PROGRAM_OFFICER_NAME = `Program Official Information`,
      STUDY_SECTION_NAME = `Study Section`,  # 2025 manual download has name, not code of the section (ExPORTER data has both)
      # Activity
      ACTIVITY = `Activity`,
      # PI info
      PI_IDS = `Contact PI Person ID`,
      PI_NAMEs = `Contact PI / Project Leader`,
      # Organization
      ORG_NAME = `Organization Name`,
      ORG_CITY = `Organization City`,
      ORG_STATE = `Organization State`,
      ORG_COUNTRY = `Organization Country`,
      ORG_ZIPCODE = `Organization Zip`,
      ORG_DISTRICT = `Congressional District`,
      ORG_DEPT = `Department`,
      ORG_DUNS = `DUNS Number`,
      ORG_FIPS = `FIPS`,
      ORG_IPF_CODE = `Organization ID (IPF)`,
      ED_INST_TYPE = `Organization Type`,
      # Project content
      PROJECT_TITLE = `Project Title`,
      PROJECT_TERMS = `Project Terms`,
      PHR = `Public Health Relevance`,
      NIH_SPENDING_CATS = `NIH Spending Categorization`,
      ABSTRACT_TEXT = `Project Abstract`,
      # Dates
      FISCAL_YEAR = `Fiscal Year`,
      PROJECT_START = `Project Start Date`,
      PROJECT_END = `Project End Date`,
      AWARD_NOTICE_DATE = `Award Notice Date`,
      BUDGET_START = `Budget Start Date`,
      BUDGET_END = `Budget End Date`,
      # Budget
      TOTAL_COST = `Total Cost`,
      TOTAL_COST_SUB_PROJECT = `Total Cost(Sub Projects)`,
      DIRECT_COST_AMT = `Direct Cost IC`,
      INDIRECT_COST_AMT = `Indirect Cost IC`,
      # Other
      ARRA_FUNDED = `ARRA Indicator`,
      CFDA_CODE = `Assistance Listing Number`
    )

  # Convert date columns from character (m/d/Y format) to Date
  df <- df %>%
    mutate(
      PROJECT_START = mdy(PROJECT_START),
      PROJECT_END = mdy(PROJECT_END),
      BUDGET_START = mdy(BUDGET_START),
      BUDGET_END = mdy(BUDGET_END),
      AWARD_NOTICE_DATE = mdy(AWARD_NOTICE_DATE)
    )

  # Ensure SUBPROJECT_ID is character for consistency with ExPORTER
  df <- df %>%
    mutate(SUBPROJECT_ID = as.character(SUBPROJECT_ID))

  # Extract CORE_PROJECT_NUM from FULL_PROJECT_NUM
  # FULL_PROJECT_NUM format: "3R01FD007630-01S4" -> CORE_PROJECT_NUM: "R01FD007630"
  df <- df %>%
    mutate(
      CORE_PROJECT_NUM = str_extract(FULL_PROJECT_NUM, "[A-Z][0-9]{2}[A-Z]{2}[0-9]+")
    )

  cat("  Read", basename(file_path), ":", nrow(df), "rows\n")
  return(df)
}

# Read all manual 2025 files
nih_2025_list <- lapply(manual_2025_files, read_manual_2025)

# Combine all 2025 data
nih_2025_all <- bind_rows(nih_2025_list)

cat("\nManual 2025 data combined:\n")
cat("  Total rows:", nrow(nih_2025_all), "\n")
cat("  Total columns:", ncol(nih_2025_all), "\n")
cat("  Unique grants:", n_distinct(nih_2025_all$CORE_PROJECT_NUM), "\n")

# Clean up
rm(nih_2025_list)
} # end skip_2025_processing check
```

## Combine NIH Exporter data from 2014-2024 with 2025 data

```{r combine-manual-with-exporter}
# Combine 2025 manual data with ExPORTER bulk data (2014-2024)
# Data was already loaded in load-cache section if cache exists

if (skip_2025_processing) {
  cat("Using combined 2014-2025 cache (already loaded above):\n")
  cat("  Total rows:", nrow(nih_exporter_usa), "\n")
  cat("\nGrants per fiscal year:\n")
  print(table(nih_exporter_usa$fiscal_year))
} else {
  # Combine 2025 manual data with ExPORTER bulk data
  cat("ExPORTER columns:", ncol(nih_exporter_usa), "\n")
  cat("Manual 2025 columns:", ncol(nih_2025_all), "\n")

  common_cols_2025 <- intersect(names(nih_exporter_usa), names(nih_2025_all))
  cat("Common columns between ExPORTER and 2025 manual:", length(common_cols_2025), "\n")
  cat("Common columns:\n")
  print(common_cols_2025)

  # Ensure consistent column types before binding
  # Some columns are character in one dataset and numeric in another
  cols_to_character <- c("PI_IDS", "CFDA_CODE", "SUBPROJECT_ID", "SUFFIX",
                         "SUPPORT_YEAR", "APPLICATION_TYPE", "ORG_DUNS",
                         "ORG_FIPS", "ORG_ZIPCODE", "ORG_DISTRICT", "SERIAL_NUMBER")

  for (col in cols_to_character) {
    if (col %in% names(nih_exporter_usa)) {
      nih_exporter_usa[[col]] <- as.character(nih_exporter_usa[[col]])
    }
    if (col %in% names(nih_2025_all)) {
      nih_2025_all[[col]] <- as.character(nih_2025_all[[col]])
    }
  }

  # Add 2025 data to ExPORTER USA data
  nih_exporter_usa <- bind_rows(
    nih_exporter_usa,
    nih_2025_all %>% select(all_of(common_cols_2025))
  )

  # Convert all column names to lowercase for easier typing
  names(nih_exporter_usa) <- tolower(names(nih_exporter_usa))

  cat("\nCombined ExPORTER + 2025 manual data:\n")
  cat("  Total rows:", nrow(nih_exporter_usa), "\n")
  cat("\nGrants per fiscal year (including 2025):\n")
  print(table(nih_exporter_usa$fiscal_year))

  # Clean up
  rm(nih_2025_all)
  gc()

  # Save combined cache
  saveRDS(nih_exporter_usa, combined_cache)
  cat("Saved combined cache to:", combined_cache, "\n")
}
```

## Read Grant Witness NIH terminations

```{r read-grant-witness}
# SOURCE: Grant Witness - https://www.grant-witness.us
nih_cut <- read_csv(here('data/grant_witness_nih_terminations.csv'), show_col_types = FALSE)

cat("Grant Witness NIH terminations:\n")
cat("  Rows:", nrow(nih_cut), "\n")
cat("  Columns:", ncol(nih_cut), "\n")
```

## Column Analysis and Selection

### All ExPORTER Columns (48 columns) - Authoritative source for all grants

**Identifiers:**
- application_id, core_project_num (join key), full_project_num
- serial_number, subproject_id, suffix, support_year

**Grant metadata:**
- activity, application_type, funding_mechanism, cfda_code, foa_number

**Agency/Admin (Institute or Center = IC):**
- administering_ic, ic_name, funding_ics, program_officer_name
- study_section, study_section_name

**Organization:**
- org_name, org_city, org_state, org_country, org_dept, org_zipcode
- org_district, org_duns, org_fips, org_ipf_code, ed_inst_type

**PI:** pi_ids, pi_names

**Project:** project_title, project_terms, phr, nih_spending_cats

**Dates:** fiscal_year, project_start, project_end, budget_start, budget_end, award_notice_date

**Budget:** total_cost, direct_cost_amt, indirect_cost_amt, total_cost_sub_project

### All Grant Witness Columns (56 columns) - Termination-specific data

**Overlapping with ExPORTER (use ExPORTER version):**
- project_title, activity_code, org_name, org_state, org_city, terms
- phr_text, study_section, foa, total_award

**Unique termination-specific columns to KEEP:**
- status, ever_frozen, frozen_date, unfrozen_date
- termination_date, targeted_start_date, targeted_end_date
- reinstatement_indicator, reinstated_est_date, reinstatement_case
- last_payment_date, cancellation_source
- total_estimated_outlays, total_estimated_remaining
- file_c_outlays (detailed outlay history)
- flagged_words (analysis of concerning terms)
- us_rep, us_rep_phone, org_congdist (political data)
- court_reported, court_restoration_url (legal info)
- usaspending_url, taggs_url, reporter_url (reference links)

## Select columns for analysis dataset

```{r column-recommendation}
# Recommended columns from each dataset

# From ExPORTER (authoritative grant data)
# Using column names that exist in both bulk download AND manual 2025 data
exporter_cols <- c(
  # IDs
  "application_id", "core_project_num", 
  # Agency
  "administering_ic", "ic_name", "study_section", "study_section_name",
  # PI
  "pi_names",
  # Organization
  "org_name", "org_state", "org_district",
  "org_dept", "ed_inst_type", #educational institution type
  # Project
  "project_title", "project_terms", "phr", "nih_spending_cats",
  "abstract_text",
  # Dates
  "fiscal_year", "project_start", "project_end",
  # Budget
  "total_cost"
)

# From Grant Witness (termination-specific only)
grantwitness_cols <- c(
  # Join key
  "core_award_number",
  # Status
  "status", "ever_frozen",
  # Termination dates
  "frozen_date", "unfrozen_date", "termination_date",
  "targeted_start_date", "targeted_end_date",
  # Reinstatement
  "reinstatement_indicator", "reinstated_est_date",
  # Spending
  "total_estimated_outlays", "total_estimated_remaining",
  "last_payment_date", "cancellation_source",
  # Analysis
  "flagged_words",
  # Political
  "us_rep",
  # URLs
  "usaspending_url", "reporter_url"
)

cat("Selected ExPORTER columns:", length(exporter_cols), "\n")
cat("Selected Grant Witness columns:", length(grantwitness_cols), "\n")
```

## Test Join Strategy

Join on CORE_PROJECT_NUM (ExPORTER) = core_award_number (Grant Witness)

```{r test-join}
# Join statistics
matched <- nih_cut %>%
  semi_join(nih_exporter_usa, by = c("core_award_number" = "core_project_num"))

unmatched <- nih_cut %>%
  anti_join(nih_exporter_usa, by = c("core_award_number" = "core_project_num"))

cat("Join results:\n")
cat("  Total cut grants:", nrow(nih_cut), "\n")
cat("  Matched in ExPORTER:", nrow(matched), "(", round(100*nrow(matched)/nrow(nih_cut), 1), "%)\n")
cat("  Unmatched (likely FY 2025):", nrow(unmatched), "\n")
```

## Create Joined Analysis Dataset

Strategy: ExPORTER is authoritative. Keep most recent FY record per grant.
Add termination info from Grant Witness where available.

```{r create-joined-dataset}
# Get most recent ExPORTER record per grant
nih_exporter_latest <- nih_exporter_usa %>%
  select(all_of(exporter_cols)) %>%
  group_by(core_project_num) %>%
  filter(fiscal_year == max(fiscal_year)) %>%
  slice(1) %>%  # In case of ties, take first
  ungroup()

cat("ExPORTER deduplicated:", nrow(nih_exporter_latest), "unique grants\n")

# Select columns from Grant Witness (termination-specific only)
nih_cut_slim <- nih_cut %>%
  select(all_of(grantwitness_cols))

# Join: all grants from ExPORTER, add termination info where available
nih_joined <- nih_exporter_latest %>%
  left_join(nih_cut_slim, by = c("core_project_num" = "core_award_number")) %>%
  mutate(status = str_remove(status, "^\\S+\\s+")) %>%  # remove emoji from status
  arrange(desc(!is.na(status))) |> # put cut grants first
  rename(
    #make column names self-evident
    organization_type = "ed_inst_type",
    public_health_relevance = 'phr',
    congressional_district = 'org_district',
    nih_spending_categorization = 'nih_spending_cats',
    project_abstract = 'abstract_text', 
    administering_ic_name = 'ic_name' # in parallel to short code administering_ic
  ) %>%
  # Group date columns together for easy comparison
  relocate(fiscal_year, project_start, project_end, targeted_start_date,
           targeted_end_date, frozen_date, unfrozen_date, termination_date,
           reinstatement_indicator, reinstated_est_date, last_payment_date,
           .after = project_abstract) %>%
  # Group budget columns together
  relocate(total_cost, total_estimated_outlays, total_estimated_remaining,
           .after = reinstated_est_date)

cat("\nJoined dataset:\n")
cat("  Total rows:", nrow(nih_joined), "\n")
cat("  Total columns:", ncol(nih_joined), "\n")
cat("  Cut grants:", sum(!is.na(nih_joined$status)), "\n")
cat("  Non-cut grants:", sum(is.na(nih_joined$status)), "\n")
```

```{r check-lost-grants}
# Which cut grants were lost in the join (not in ExPORTER)?
lost_cuts <- nih_cut %>%
  anti_join(nih_exporter_usa, by = c("core_award_number" = "core_project_num"))

cat("Cut grants not found in ExPORTER:", nrow(lost_cuts), "\n")

# Show sample
if (nrow(lost_cuts) > 0) {
  cat("\nSample lost grants:\n")
  print(head(lost_cuts %>% select(core_award_number, project_title, status), 10))
}

# Checked some of them: One lasted only one day in 2024, one got cut before it started, one was from Vietnam - probably ok to lose these 24 grants
```

```{r save-analysis-dataset}
# Save joined dataset
write_csv(nih_joined, here('data_analysis/nih_analysis.csv'))
arrow::write_parquet(nih_joined, here('data_analysis/nih_analysis.parquet'))
cat("NIH analysis dataset:", nrow(nih_joined), "grants,", ncol(nih_joined), "columns\n")
cat("Saved to data_analysis/nih_analysis.csv and .parquet\n")
```


